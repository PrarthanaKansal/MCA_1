# -*- coding: utf-8 -*-
"""Correlogram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uprx-nLvqtaYkdbIiD8zuFpII_kvXtZV
"""

from google.colab import drive
import os
import torch, matplotlib
import numpy as np
import imageio
import matplotlib.pyplot as plt
import cv2
import tensorflow as tf
import glob
from google.colab.patches import cv2_imshow
import PIL 
from PIL import Image  
from google.colab import files
import pickle
import pandas as pd
import time
 
image_path = "/content/drive/My Drive/images"
drive.mount('/content/drive')
def loadImages(path):
    '''Put files into lists and return them as one list with all images 
     in the folder'''
    image_files = sorted([os.path.join(path, '', file)
                          for file in os.listdir(path)
                          if file.endswith('.jpg')])
    return image_files

def loadQueryFolder():
  #Gives the array of all text files in the query folder
  file_content =[]
  txt_files = glob.glob("/content/drive/My Drive/train/query/*.txt")
  return txt_files 

def query():
  #Takes one file from the query folder and extracts the image name
  Query = loadQueryFolder()
  listOfQueries=[]
  for i in range(len(Query)):
    with open(i, 'r') as fd:
      line = fd.readline()
      #b = line.index(" ")
      query = line
      query+=".jpg"
    # line = open(i,"r").read()
    # b = line.index(" ")
    # query = line[5:b]
    # query+=".jpg"
      listOfQueries.append(query)
  return listOfQueries

def display_one(a, title1 = "Original"):
    plt.imshow(a), plt.title(title1)
    plt.show()

def get_n(n,x,y,color,img_no_dither,X,Y):
    valid_vector =  [  0 for i in range(8*n) ]
    positive_count=0
    total_count=0
    nbrs_x =  [  0 for i in range(8*n) ]
    nbrs_y = [  0 for i in range(8*n) ]
    nbrs_y[0]=y
    d = 1
    for k in range(1,n+1):
        nbrs_y[k]=y-d
        d=d+1
    
    for i in range(n+1,3*n+1):
      nbrs_y[i]=y-n

    d = 0
    for k in range(3*n,5*n+1):
          nbrs_y[k]=y-n+d;
          d=d+1;
    
    
    for i in range(5*n,7*n+1):
      nbrs_y[i]=y+n
    
    d = 0
    for i in range(7*n,7*n+1+(n-1)):
      nbrs_y[i]=y+n-d
      d=d+1

  #-------------------------------------------------
  #-------------------------------------------------
    nbrs_x[0]=x-n
    for i in range(1,n+1):
      nbrs_x[i]=x-n
    
    d =0
    for k in range(1+n,3*n+1):
        nbrs_x[k]=x-n+d
        d=d+1
    for i in range(3*n,5*n+1):
      nbrs_x[i]=x+n

    d=0;
    for k in range(5*n,7*n+1):
      nbrs_x[k]=x+n-d
      d=d+1
    
    for i in range(7*n,7*n+1+(n-1)):
      nbrs_x[i]=x-n
    
    for i in range(0,8*n):
        if nbrs_x[i]>=0 and nbrs_x[i]<X and nbrs_y[i]>=0 and nbrs_y[i]<Y:
            valid_vector[i]=1;
          
        else:
            valid_vector[i]=0;
    for j in range(0,8*n):
        if valid_vector[j]==1:
          data= img_no_dither[nbrs_y[j]][nbrs_x[j]]
          #print("------")
          if (data==color):
            positive_count=positive_count+1;
          total_count=total_count+1;
    return positive_count,total_count


def processing(I):
    im = Image.open(I)  
    newsize = (64, 64) 
    img = im.resize(newsize) 
    X= 64
    Y= 64
    #display_one(img)
    a = img.quantize(64)
    img_no_dither = np.array(a)
    #print(img_no_dither.shape)

    count_matrix = [ [ 0 for i in range(2) ] for j in range(64) ]
    total_matrix = [ [ 0 for i in range(2) ] for j in range(64) ] 
    prob_dict = {0:"",1:""}
    distance_vector = [1,3]
    for serial_no in range(0,2):
      for x in range(0,64):
        for y in range(0,64):
          color = img_no_dither[y][x]
          # print("y", y)
          # print("Color",color)
          positive_count,total_count =get_n(distance_vector[serial_no],x,y,color,img_no_dither,64,64);
          count_matrix[color][serial_no]=count_matrix[color][serial_no]+positive_count;
          total_matrix[color][serial_no]=total_matrix[color][serial_no]+total_count; 
      num = [col[serial_no] for col in count_matrix] 
      den = [col[serial_no]+1 for col in total_matrix]
      res = [col[serial_no] for col in count_matrix]
      for i in range(len(num)):
          res[i]=float(float(num[i])/float(den[i]))
      prob_dict[serial_no]=res
    
    # for serial_no in range(0,2):
    #   correlogram_vector=np.concatenate((correlogram_vector,prob_dict[serial_no]))
    correlogram_vector = [ [ prob_dict[0][i] for i in range(len(prob_dict[0])) ] for j in range(2) ]
    
    #print(len(correlogram_vector))
    for i in range(2):
      correlogram_vector[i]=prob_dict[i]
    # print(correlogram_vector)
    return correlogram_vector

def getCorrelogramOfAllImages(dataset):
    corrDict={}
    for i in range(len(dataset)):
      # print("INSIDE")
      a = processing(dataset[i])
      a = np.array(a)
      # print("arrayed")
      a = a.flatten()
      #print(processing(dataset[i]))
      # a = np.asarray(processing(dataset[i]))
      
      # a = a.flatten()
      # print("FLATTENED")
      corrDict[dataset[i][31:]]=a
      print("Calculating feature of image ",i)
    m = 64
    d = 2
    cols = ['feature'+str(i+1) for i in range(m*d)]
    corrDict = pd.DataFrame.from_dict(corrDict, orient='index', columns=cols)
    # print(corrDict)

    pickle.dump(corrDict, open('Dict64.pkl', 'wb'))
    print("Download Started")
    files.download('Dict64.pkl')
    print("Download ended")
    return corrDict

def similarity(m, query_corr, sample_corr):
  #return similarity between correlograms of 2 images
  temp =[]
  for a, b in zip(query_corr, sample_corr):
    num = abs(a-b)
    den = (1+a+b)
    final = (1/m)*(num/den)
    temp.append(final)
  ans = sum(temp)
  return ans
    
  # return ((1/m)*(sum([(abs(a-b)/(1+a+b)) for a, b in zip(query_corr, sample_corr)])))

def sort_score(l):
    l.sort(key = lambda x: x[1], reverse = True)
    return l


# features = pickle.load(open('Dict64.pkl', 'rb'))


#m = 128
#d = 3
#cols = ['feature'+str(i+1) for i in range(m*d)]
#features = pd.DataFrame.from_dict(features, orient='index', columns=cols)




def main():
    global image_path
    dataset = loadImages(image_path)
    # a = processing(dataset[0])
    # print(a)
    # a =np.array(a)
    # print(a)
    # a = a.flatten()
    # print(a)
    #print(processing(dataset[0]))
    #dataset = dataset[0]
    #print(dataset)
    # print(dataset)
    # for i in range(3):
    #     processing(dataset[i])
    # dataset = dataset[0:2]



    corrDict = getCorrelogramOfAllImages(dataset)
    print("Downloaded")
    query_dir = os.fsencode(str('/content/drive/My Drive/train/query'))
    query = []

    good_percent = []
    junk_percent = []
    ok_percent = []

    precision = []
    recall = []
    f1 = []

    retrieval_time = []

    for f in os.listdir(query_dir):
        start = time.time()  #start time of retrieval
        file_name = os.fsdecode(f)
        file = open('/content/drive/My Drive/train/query/'+file_name)
        line = file.readline()
        file.close()
        name = line.split(" ")[0][5:]
        fullName = name+'.jpg'
        query.append(fullName)
        query_corr = corrDict.loc[fullName].values.tolist()
        scores = [(img, similarity(64,query_corr, corrDict.loc[img].values.tolist())) for img in corrDict.index if img != str(name+'.jpg')]
        scores = sort_score(scores)
      
        toOpenGood = '/content/drive/My Drive/train/ground_truth/'+file_name[:-9]+'good.txt'
        toOpenJunk = '/content/drive/My Drive/train/ground_truth/'+file_name[:-9]+'junk.txt'
        toOPenOk = '/content/drive/My Drive/train/ground_truth/'+file_name[:-9]+'ok.txt'

        good_images = [line.rstrip('\n') for line in open(toOpenGood)]
        junk_images = [line.rstrip('\n') for line in open(toOpenJunk)]
        ok_images = [line.rstrip('\n') for line in open(toOPenOk)]
        
        a = len(good_images)
        b = len(junk_images)
        c = len(ok_images)
        n = a+b+c #top n images to be retrieved
        top_n = [a[0] for a in scores][:n]
        good_retrieved = [a for a in top_n if a[:-4] in good_images]
        junk_retrieved = [a for a in top_n if a[:-4] in junk_images]
        ok_retrieved = [a for a in top_n if a[:-4] in ok_images]
        d = len(good_retrieved)
        e = len(junk_retrieved)
        f = len(ok_retrieved)
        
        # n = a+b+c #top n images to be retrieved
        # # n = len(good_images) + len(junk_images) + len(ok_images) #top n images to be retrieved
      
        # top_n = [a[0] for a in scores][:n]
      
        end = time.time()  #end time of retrieval
      
        # good_retrieved = [a for a in top_n if a[:-4] in good_images]
        # junk_retrieved = [a for a in top_n if a[:-4] in junk_images]
        # ok_retrieved = [a for a in top_n if a[:-4] in ok_images]
      
        good_percent.append(d*100/a)
        junk_percent.append(e*100/b)
        ok_percent.append(f*100/c)
      
        precision.append((d+e+f)/n)
        recall.append((d+e+f)/n)

        f1Num = 2*precision[-1]*recall[-1]
        f1Den = precision[-1]+recall[-1]+1
        f1.append(f1Num/f1Den)
      
        retrieval_time.append(end-start)

      
      
    print("Maximum Precision", max(precision))
    print("Minimum Precision", min(precision))
    print("Average Precision", mean(precision))
    print() 
    print("Maximum Recall", max(recall))
    print("Minimum Recall", min(recall))
    print("Average Recall", mean(recall))
    print() 
    print("Maximum F1 Score", max(f1))
    print("Minimum F1 Score", min(f1))
    print("Average F1 Score", mean(f1))
    print() 
    print("Average Retrieval Time", mean(retrieval_time))
    print("Average Good Retrieved", mean(good_percent))
    print("Average Junk Retrieved", mean(junk_percent))
    print("Average Ok Retrieved", mean(ok_percent)) 
    
    
    
    
main()

from statistics import mean
from google.colab import drive
import os
import torch, matplotlib
import numpy as np
import imageio
import matplotlib.pyplot as plt
import cv2
import tensorflow as tf
import glob
from google.colab.patches import cv2_imshow
import PIL 
from PIL import Image  
from google.colab import files
import pickle
import pandas as pd
import time
drive.mount('/content/drive')
def temp():
    dbfile = open('/content/drive/My Drive/Dict64.pkl', 'rb')      
    corrDict = pickle.load(dbfile) 

    query_dir = os.fsencode(str('/content/drive/My Drive/train/query'))
    query = []

    good_percent = []
    junk_percent = []
    ok_percent = []

    precision = []
    recall = []
    f1 = []

    retrieval_time = []

    for f in os.listdir(query_dir):
        start = time.time()  #start time of retrieval
        file_name = os.fsdecode(f)
        file = open('/content/drive/My Drive/train/query/'+file_name)
        line = file.readline()
        file.close()
        name = line.split(" ")[0][5:]
        fullName = name+'.jpg'
        query.append(fullName)
        query_corr = corrDict.loc[fullName].values.tolist()
        scores = [(img, similarity(64, query_corr, corrDict.loc[img].values.tolist())) for img in corrDict.index if img != str(name+'.jpg')]
        scores = sort_score(scores)
      
        toOpenGood = '/content/drive/My Drive/train/ground_truth/'+file_name[:-9]+'good.txt'
        toOpenJunk = '/content/drive/My Drive/train/ground_truth/'+file_name[:-9]+'junk.txt'
        toOPenOk = '/content/drive/My Drive/train/ground_truth/'+file_name[:-9]+'ok.txt'

        good_images = [line.rstrip('\n') for line in open(toOpenGood)]
        junk_images = [line.rstrip('\n') for line in open(toOpenJunk)]
        ok_images = [line.rstrip('\n') for line in open(toOPenOk)]

        a = len(good_images)
        b = len(junk_images)
        c = len(ok_images)
        n = a+b+c #top n images to be retrieved
        top_n = [a[0] for a in scores][:n]

        good_retrieved = [a for a in top_n if a[:-4] in good_images]
        junk_retrieved = [a for a in top_n if a[:-4] in junk_images]
        ok_retrieved = [a for a in top_n if a[:-4] in ok_images]

        d = len(good_retrieved)
        e = len(junk_retrieved)
        f = len(ok_retrieved)

        
      
        
      
        end = time.time()  #end time of retrieval
      
        
      
        good_percent.append(d*100/a)
        junk_percent.append(e*100/b)
        ok_percent.append(f*100/c)
      
        precision.append((d+e+f)/n)
        recall.append((d+e+f)/n)
        f1Num = 2*precision[-1]*recall[-1]
        f1Den = precision[-1]+recall[-1]+1
        f1.append(f1Num/f1Den)
      
        retrieval_time.append(end-start)

      
    print("Maximum Precision", max(precision))
    print("Minimum Precision", min(precision))
    print("Average Precision", mean(precision))
    print() 
    print("Maximum Recall", max(recall))
    print("Minimum Recall", min(recall))
    print("Average Recall", mean(recall))
    print() 
    print("Maximum F1 Score", max(f1))
    print("Minimum F1 Score", min(f1))
    print("Average F1 Score", mean(f1))
    print() 
    print("Average Retrieval Time", mean(retrieval_time))
    print("Average Good Retrieved", mean(good_percent))
    print("Average Junk Retrieved", mean(junk_percent))
    print("Average Ok Retrieved", mean(ok_percent)) 
    
    
   
    
temp()

